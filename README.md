# Data Processing Block

The Data Processing Block is a Python library that provides the core logic for the processing of data coming in streaming (collected by sensors in real time) or extracted from the database of historical data stored in the application. Therefore its functioning is based on two core files: "streaming_pipeline.py" for the streaming data and "on_request_pipeline.py" that allows to perform forecasting or data transformation based on historical data whenever the user requests it from the GUI.



## ğŸ“ Repository Contents

The repository contains the following files and directories

```
ğŸ“‚ data-preprocessing-
â”œâ”€â”€ ğŸ“‚ data
|   â”œâ”€â”€ â› alerts_configuration.json
â”‚   â”œâ”€â”€ â› cleaned_predicted_dataset.json
â”‚   â”œâ”€â”€ â› historical_dataset.json
â”‚   â”œâ”€â”€ â› original_adapted_dataset.json
â”‚   â””â”€â”€ âŠ store.pkl
â”œâ”€â”€ ğŸ“‚ docs
â”‚   â””â”€â”€ ğŸ“– index.html
â”œâ”€â”€ ğŸ“‚ src
â”‚   â”œâ”€â”€ ğŸ“‚ app
|       â”œâ”€â”€ ğŸ“‚ notification
â”‚       â”‚   â””â”€â”€ ğŸ“¤ mail_sender.py
â”‚       â”œâ”€â”€ ğŸ¤– config.py
â”‚       â”œâ”€â”€ ğŸ¤– connection_functions.py
â”‚       â”œâ”€â”€ ğŸ¤– dataprocessing_functions.py
â”‚       â”œâ”€â”€ ğŸ¤– fix_data.py
â”‚       â”œâ”€â”€ ğŸ¤– initialize.py
â”‚       â”œâ”€â”€ ğŸ¤– streaming_pipeline.py
â”‚       â”œâ”€â”€ ğŸŒ main.py
â”‚       â”œâ”€â”€ âš¡ on_request_pipeline.py
â”‚       â”œâ”€â”€ âš¡ streaming_pipeline.py
â”‚       â””â”€â”€ âš¡ test_myupdate.ipynb
â”œâ”€â”€ ğŸ”„ .gitignore
â”œâ”€â”€ ğŸ³ Dockerfile
â”œâ”€â”€ ğŸ“– README.md
â”œâ”€â”€ ğŸ›  poetry.lock
â””â”€â”€ ğŸ›  pyproject.toml
```

## ğŸ“ Repository Contents

The repository contains the following files and directories:

- **`ğŸ“‚ data`**  
   A directory containing datasets used in the project.
     - **`â› alerts_configuration.json`**  
      A JSON file containing the customed alerts that have been created.
   - **`â› cleaned_predicted_dataset.json`**  
      A JSON file containing the cleaned and predicted dataset.
   - **`â› historical_dataset.json`**  
      A JSON file containing historical data.\
   - **`â› original_adapted_dataset.json`**  
      A JSON file containing the original adapted dataset.
   - **`âŠ store.pkl`**  
      A pickle file for storing serialized data or models.

- **`ğŸ“‚ docs`**  
   A directory containing documentation files.
   - **`ğŸ“– index.html`**  
      An HTML file providing the project documentation.

- **`ğŸ“‚ src`**  
   A directory containing the source code.
   - **`ğŸ“‚ app`**  
      A directory containing the main application logic.
      - **`ğŸ“‚ notification`**  
         A directory with notification-related logic.
         - **`ğŸ“¤ mail_sender.py`**  
            A Python script to send notification emails.
      - **`ğŸ¤– config.py`**  
         A Python script containing configuration settings.
      - **`ğŸ¤– connection_functions.py`**  
         A Python script with functions for connecting to external services or databases.
      - **`ğŸ¤– dataprocessing_functions.py`**  
         A Python script with utility functions for data processing.
      - **`ğŸ¤– fix_data.py`**  
         A Python script for data cleaning and fixing issues.
      - **`ğŸ¤– initialize.py`**  
         A Python script that initializes the application.
      - **`ğŸ¤– streaming_pipeline.py`**  
         A Python script defining the streaming data pipeline.
      - **`ğŸŒ main.py`**  
         The main entry point for the application.
      - **`âš¡ on_request_pipeline.py`**  
         A Python script defining the pipeline for processing incoming requests.
      - **`âš¡ streaming_pipeline.py`**  
         Another Python script for handling streaming data pipelines.
     - **`âš¡ test_myupdate.ipynb`**  
      THIS IS THE FILE TO RUN TO TEST THE NEW FUNCTIONALITIES.

- **`ğŸ”„ .gitignore`**  
   A file specifying which files Git should ignore.
- **`ğŸ³ Dockerfile`**  
   A file containing the Docker configuration to containerize the application.
- **`ğŸ“– README.md`**  
   The README file containing information about the project, setup instructions, and more.
- **`ğŸ›  poetry.lock`**  
   A lock file generated by Poetry to ensure consistent dependency versions.
- **`ğŸ›  pyproject.toml`**  
   The Poetry configuration file specifying project dependencies and metadata.


---

## ğŸš€ Getting Started

### Prerequisites

- Docker should be installed on your machine.
- Git should be installed on your machine.
- The [database](https://github.com/Kreative-Performative-Individuals/smart-industrial-database) container should be running

---

### Cloning the Repository

Clone this repository to your local machine running

```bash
git clone https://github.com/Kreative-Performative-Individuals/data-preprocessing-.git
cd data-preprocessing-
```

This will create a new directory named `data-preprocessing-` in your current working directory and navigate you into it.

### Docker Instructions

Build and run the container using the following commands

```bash
docker build --tag data_preprocessing .
```

```bash
 docker run --rm --name data-preprocessing- -p 8003:8003 data_preprocessing 
```

This command will start a new Docker container named `data-preprocessing-` and expose the application on port `8000`.

You can now access the Data Preprocessing API by visiting `http://localhost:8000` in your web browser or using tools like Postman.

To stop the running container, run 

```bash
docker stop data-preprocessing-
```

Since once stopped, the container is automatically deleted by `--rm`, we can just delete its image

```bash
docker rmi data_preprocessing
```



---

